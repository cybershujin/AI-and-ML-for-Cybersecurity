# Vendors, tools, practices around detection attacks against AI and LLMs



Open source

- [Vigil - security scanner for LLM prompts / Jailbreak attempts](https://github.com/deadbits/vigil-llm)

- [Rebuff - security scanner for LLM prompts / Jailbreak attempts](https://github.com/protectai/rebuff)

  